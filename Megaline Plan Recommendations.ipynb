{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megaline Plan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to aid Megaline, a mobile carrier, in developing a model that will analyze subscribers' behavior. Once the behavior is analyzed, subscribers on a legacy plan can be recommended one of Megaline's newer plans: Smart or Ultra. We are provided with behavior data from subscribers who have already switched to the new plans. A successful model will classify a correct new plan to a legacy plan customer. We will be working with data that we've used previously, so the data will be clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Planning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand that this is a classification problem, as we are determining which plan to recommend: Surf or Ultra. Since we are concerned with accuracy, we will need to increase the total number of correct plans picked. Consequently, we will likely go with a random forest model, as speed is not a crucial factor. We determine the minimum accuracy threshold to be 0.75, meaning we correctly recommend at least 75% of customers. Since we do not have a separate test dataset, we will split our source data to create a validating and test dataset dataset, each with the conventional 20% of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly_express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from plotly_express) (5.4.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.9/site-packages (from plotly_express) (1.21.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.9/site-packages (from plotly_express) (0.5.2)\n",
      "Requirement already satisfied: scipy>=0.18 in /opt/conda/lib/python3.9/site-packages (from plotly_express) (1.8.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from plotly_express) (0.13.2)\n",
      "Requirement already satisfied: pandas>=0.20.0 in /opt/conda/lib/python3.9/site-packages (from plotly_express) (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.20.0->plotly_express) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.20.0->plotly_express) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from patsy>=0.5->plotly_express) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from plotly>=4.1.0->plotly_express) (8.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.9/site-packages (from statsmodels>=0.9.0->plotly_express) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.9.0->plotly_express) (2.4.7)\n",
      "Installing collected packages: plotly-express\n",
      "Successfully installed plotly-express-0.4.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user -U plotly_express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "# import plotly_express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe\n",
    "df = pd.read_csv('/datasets/users_behavior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>122.0</td>\n",
       "      <td>910.98</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35124.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>25.0</td>\n",
       "      <td>190.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3275.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>97.0</td>\n",
       "      <td>634.44</td>\n",
       "      <td>70.0</td>\n",
       "      <td>13974.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>64.0</td>\n",
       "      <td>462.32</td>\n",
       "      <td>90.0</td>\n",
       "      <td>31239.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>80.0</td>\n",
       "      <td>566.09</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29480.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      calls  minutes  messages   mb_used  is_ultra\n",
       "0      40.0   311.90      83.0  19915.42         0\n",
       "1      85.0   516.75      56.0  22696.96         0\n",
       "2      77.0   467.66      86.0  21060.45         0\n",
       "3     106.0   745.53      81.0   8437.39         1\n",
       "4      66.0   418.74       1.0  14502.75         0\n",
       "...     ...      ...       ...       ...       ...\n",
       "3209  122.0   910.98      20.0  35124.90         1\n",
       "3210   25.0   190.36       0.0   3275.61         0\n",
       "3211   97.0   634.44      70.0  13974.06         0\n",
       "3212   64.0   462.32      90.0  31239.78         0\n",
       "3213   80.0   566.09       6.0  29480.52         1\n",
       "\n",
       "[3214 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column types to integer\n",
    "df.calls = df.calls.astype('int')\n",
    "df.messages = df.messages.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   int64  \n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   int64  \n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Confirm data type change, look at data summary\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calls       0\n",
       "minutes     0\n",
       "messages    0\n",
       "mb_used     0\n",
       "is_ultra    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure no missing values \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure no duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2229\n",
       "1     985\n",
       "Name: is_ultra, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values counts of each plan, 1 is Ultra\n",
    "df.is_ultra.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into 3 \n",
    "features = df.drop(['is_ultra'], axis=1)\n",
    "target = df['is_ultra']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split( \n",
    "    features, target, test_size=0.2, random_state=19) # split 20% of data to make validation set\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train, target_train, test_size=0.25, random_state=19) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1928, 4)\n",
      "(1928,)\n",
      "(643, 4)\n",
      "(643,)\n",
      "(643, 4)\n",
      "(643,)\n"
     ]
    }
   ],
   "source": [
    "# Visual of the split data\n",
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(target_valid.shape)\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Depth: 8 ,  Accuracy of the best model on the validation set: 0.8870942657868514\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with loop for depth\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_depth = 0\n",
    "for depth in range(1,101):\n",
    "    model = DecisionTreeClassifier(random_state=19, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    result = accuracy_score(target_valid, predictions_valid) ** 0.5 # calculate RMSE on validation set\n",
    "    if result > best_result:\n",
    "        best_model = model\n",
    "        best_result = result\n",
    "        best_depth = depth  \n",
    "\n",
    "        \n",
    "print(\"Best Depth:\", best_depth, \",\"   \"  Accuracy of the best model on the validation set:\", best_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Training set: 0.8864343497199751\n",
      "Test set: 0.71850699844479\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of decision tree\n",
    "train_predictions = model.predict(features)\n",
    "valid_predictions = model.predict(features_valid)\n",
    "print('Accuracy')\n",
    "print('Training set:', accuracy_score(target, train_predictions))\n",
    "print('Validation set:', accuracy_score(target_valid, valid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set was accurate, yet the model was not so accurate making predictions on the validation set. Consequently, we will try another model to see if we can achieve higher accuracy values than our decision tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model3 = LogisticRegression(random_state=19, solver='liblinear')  \n",
    "model3.fit(features_train, target_train)  \n",
    "score_train = model3.score(features_train, target_train)  \n",
    "score_valid = model3.score(features_valid, target_valid)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Training set: 0.7429993777224643\n",
      "Validation set: 0.7107309486780715\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \n",
    "train_predictions3 = model3.predict(features)\n",
    "valid_predictions3 = model3.predict(features_valid)\n",
    "print('Accuracy')\n",
    "print('Training set:', accuracy_score(target, train_predictions3))\n",
    "print('Validation set:', accuracy_score(target_valid, valid_predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logistic regression model is less accurate than the decision tree in terms of training set accuracy. However, validation set accuracy is higher. We will try one more model to see if we can get better scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth1 = 0\n",
    "for est in range(10, 101, 10):\n",
    "    for depth in range (1, 101):\n",
    "        model1 = RandomForestClassifier(random_state=19, n_estimators=est, max_depth=depth)\n",
    "        model1.fit(features_train, target_train) # train model on training set\n",
    "        predictions_valid1 = model1.predict(features_valid) # get model predictions on validation set\n",
    "        result1 = accuracy_score(target_valid, predictions_valid1) ** 0.5 # calculate RMSE on validation set\n",
    "        if result1 > best_result:\n",
    "            best_model = model1\n",
    "            best_result = result1\n",
    "            best_est = est\n",
    "            best_depth1 = depth\n",
    "\n",
    "print(\"Accuracy of the best model on the validation set:\", best_result, \"n_estimators:\", best_est, \"best_depth:\", best_depth1)\n",
    "\n",
    "final_model = RandomForestClassifier(random_state=19, n_estimators=best_est, max_depth=best_depth1) # change n_estimators to get best model\n",
    "final_model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "final_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "train_predictions1 = final_model.predict(features)\n",
    "valid_predictions1 = final_model.predict(features_valid)\n",
    "print('Accuracy')\n",
    "print('Training set:', accuracy_score(target, train_predictions1))\n",
    "print('Validation set:', accuracy_score(target_valid, valid_predictions1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the most accurate among the training and validation sets. We  will move forward with this model and calculate more metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Accuracy \n",
    "test_predictions1 = final_model.predict(features_test)\n",
    "print('Accuracy')\n",
    "print('Test set:', accuracy_score(target_test, test_predictions1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model on the test set is 80%, which is fair. This is a general indication of the model's overall performance. Since it meets our accuracy threshold of 75%, we consider the model appropriate for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Accuracy\n",
    "max(target_test.mean(), 1 - target_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null accuracy is 68.7%. This value is the accuracy obtained when always predicting the majority class. Since our null accuracy is far less than our test set accuracy, we conclude our model is better than always assuming not Ultra. This measure is a useful baseline metric to measure against our classifier accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = metrics.confusion_matrix(target_test, test_predictions1)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Figure\n",
    "fig = px.imshow(conf_matrix, text_auto=True, labels=dict(y=\"Actual\", x=\"Predicted\"),\n",
    "                x=['Not Ultra', 'Is Ultra'],\n",
    "                y=['Not Ultra', 'Is Ultra'], title='Confusion Matrix')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Heatmap(z=[[81, 120], [399, 43]], text=[['False Negatives', 'True Positives'], ['True Negatives', 'False Positives']], \n",
    "                texttemplate=\"%{text}\", textfont={\"size\":20}, x=['Not Ultra', 'Is Ultra'],\n",
    "                y=['Not Ultra', 'Is Ultra']))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix breaks down the results to define the performance of a classification algorithm. It shows us key metrics such as true negatives and true positives. True positives are observations that were predicted to be Ultra, and were actually Ultra. True negatives are observations that were predicted to not be Ultra, and were not Ultra. We see this model is better at predicting true negatives than true positives. This demonstrates that the model can more easily determine when a plan should not be Ultra, than when a plan should be Ultra. We also see the values of false negatives and positives are relatively low.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity \n",
    "# TP / TP + FN\n",
    "print('The sensitivity is:', metrics.recall_score(target_test, test_predictions1) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity evaluates how well the classifier detects positive instances, when the plan is actually Ultra. The value is 59.7%, which is fair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity\n",
    "# TN / (TN + FP)\n",
    "print('The specificity is:', 399 / float(399 + 43)* 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specificity evaluates how well the classifier detects when the plan is not Ultra, when the plan is indeed not Ultra. The value is high, at 90.27%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positive Rate\n",
    "# FP / (TN + FP)\n",
    "print('The false positives are:', 43 / float(399 + 43)* 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false positive rate measures a not Ultra prediction when an observation is actually not Ultra. This measure is relatively low, at 9.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision \n",
    "# TP / (TP + FP)\n",
    "print('The precision is:', metrics.precision_score(target_test, test_predictions1)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision evaluates how often an Ultra prediction correctly identifies an Ultra observation. This measure is 73.6%, which is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(target_test, test_predictions1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of some prediction metrics. Recall measures the ability of a classifier to correctly find all Ultra instances. The f1-score is a weighted average of precision and recall. Support is the number of actual occurrences of the classes in the dataset. We see significantly more plans that are not Ultra. This imbalance signifies a structural weakness in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating prediction probabilities for both plans\n",
    "is_ultra = (model1.predict_proba(features_test)*100)[:, 1]\n",
    "not_ultra = (model1.predict_proba(features_test)*100)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying histogram for prediction probabilities of both plans\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=not_ultra, name='Not Ultra'))\n",
    "fig.add_trace(go.Histogram(x=is_ultra, name='Is Ultra'))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of Ultra vs Not Ultra', \n",
    "    xaxis_title_text='Prediction Percent Confidence', \n",
    "    yaxis_title_text='Count', \n",
    ")    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the inner workings of the model in predicting if a plan should be Ultra. We see that most of the occurrences when the model predicts Ultra, it is only 3-7 percent sure of that prediction. Very few predictions are seen where the model is more certain than 50%. On the other hand, the model is usually 93-97 percent certain when making a prediction that the plan is not Ultra. This is the case, as the model will associate a likelihood for the prediction to either be Ultra or not Ultra. The classification with the higher likelihood determines the predicted classification, with a threshold of 0.5. Therefore, if the model makes an Ultra classification prediction with a probability of 0.51, the model will determine that the plan should be Ultra, as the probability of not Ultra is 0.49. Consequently, prediction probabilities in the mid range, 0.4 to 0.6, are not as accurate in predicting the correct classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully created a model to predict which plan a legacy customer should upgrade to. The choices were the Ultra plan, or not the Ultra plan. Our model accurately predicted the correct classes 80% of the time. Accuracy was determined to be the key metric to evaluate the model, as it considers both precision and recall. This is important because we want to ensure customer satisfaction by recommending the correct plan for their needs. The model worked best with not Ultra classifications, as it had high specificity. The precision of the model was fair, correctly predicting Ultra 73.6% of the time. The model was optimized for the number of estimators and max depth to a reasonable degree that would not make the model training extremely long. Other hyperparameters were experimented with to determine what the best model should have. Overall, the model can be improved by further increasing the range  of estimators and max depth. Also, we believe the model would work better with a more balanced dataset, so more data on customers that migrated to the Ultra plan. This could prove difficult if customers prefer to migrate to not Ultra, so this model may be close to the final product.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4f2b0cfe12c109b58467a02dc33230d9e6228c23b43020d2941c37e2a7dfdd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
